{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import imutils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial import distance as dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define Utiliy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_people(frame, net, ln, personIdx):\n",
    "    # grab the dimensions of the frame\n",
    "    (H, W) = frame.shape[:2]\n",
    "    \n",
    "    # initialize the list to store box results, confidence, coordinates, centroid\n",
    "    results = []\n",
    "    boxes = []\n",
    "    centroids = []\n",
    "    confidences = []\n",
    "    \n",
    "    # construct a blob from the input frame and then perform a forward pass to the yolo detector\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    l_output = net.forward(ln)\n",
    "\n",
    "    # loop over each of the layer outputs\n",
    "    for output in l_output:\n",
    "        # loop over each of the detections\n",
    "        for detection in output:\n",
    "            # extract the class ID and confidence\n",
    "            scores = detection[5:]\n",
    "            classID = np.argmax(scores)\n",
    "            confidence = scores[classID]\n",
    "\n",
    "            # filter the person and weak classification\n",
    "            if classID == personIdx and confidence > MIN_CONF:\n",
    "                # scale the bounding box coordinates back relative to the size of the image\n",
    "                box = detection[0:4] * np.array([W, H, W, H])\n",
    "                (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "                x = int(centerX - (width / 2))\n",
    "                y = int(centerY - (height / 2))\n",
    "                # update our list of bounding box coordinates, centroids, and confidences\n",
    "                boxes.append([x, y, int(width), int(height)])\n",
    "                centroids.append((centerX, centerY))\n",
    "                confidences.append(float(confidence))\n",
    "\n",
    "    # apply non-maxima suppression to suppress weak, overlapping\n",
    "    idxs = cv2.dnn.NMSBoxes(boxes, confidences, MIN_CONF, NMS_THRESH)\n",
    "\n",
    "    # ensure at least one detection exists\n",
    "    if len(idxs) > 0:\n",
    "        # loop over the indexes we are keeping\n",
    "        for i in idxs.flatten():\n",
    "            (x, y) = (boxes[i][0], boxes[i][1])\n",
    "            (w, h) = (boxes[i][2], boxes[i][3])\n",
    "            r = (confidences[i], (x, y, x + w, y + h), centroids[i])\n",
    "            results.append(r)\n",
    "\n",
    "    # return the list of results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min_dist(input_video, num_iter, net, ln, personIdx):\n",
    "    #Check if camera opened successfully\n",
    "    cap = cv2.VideoCapture(input_video)\n",
    "    avgh_list = []\n",
    "    # Read until video is completed\n",
    "    counter = 1\n",
    "    while(cap.isOpened()):\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        if ret == True:\n",
    "            frame = imutils.resize(frame, width=700)\n",
    "            results = detect_people(frame, net, ln, personIdx)\n",
    "            # Atleast one person should be detected\n",
    "            if len(results) > 0:\n",
    "                # Get the average height of the person in the frame\n",
    "                avgh = np.array([i[3] - i[1] for i in np.array(results)[:,1]]).mean()\n",
    "                avgh_list.append(avgh)\n",
    "                # update the counter\n",
    "                counter = counter + 1\n",
    "            # Break the loop after 50 iterations:\n",
    "            if counter == (num_iter + 1):\n",
    "                break\n",
    "        else: \n",
    "            break\n",
    "    # When everything done, release the video capture object\n",
    "    cap.release()\n",
    "    # Closes all the frames\n",
    "    cv2.destroyAllWindows()\n",
    "    # Get average height in pixels\n",
    "    min_dist = np.array(avgh_list).mean()\n",
    "    return min_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model path directory\n",
    "MODEL_PATH = \"model\"\n",
    "\n",
    "# Flag indicating if NVIDIA CUDA GPU should be used\n",
    "USE_GPU = True\n",
    "\n",
    "# Minimum probability to filter out weak detections\n",
    "MIN_CONF = 0.3\n",
    "\n",
    "# threshold when applying non-maxima suppression\n",
    "NMS_THRESH = 0.3\n",
    "\n",
    "# Wether to display the video\n",
    "display = 1\n",
    "\n",
    "# Input Video\n",
    "input_video = \"test_videos/pedestrians.mp4\"\n",
    "\n",
    "# Ouput Video\n",
    "output_file = \"output.avi\"\n",
    "\n",
    "writer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading the YOLO classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settting backend and target to CUDA...\n"
     ]
    }
   ],
   "source": [
    "# load the COCO class labels our YOLO model was trained on\n",
    "labelsPath = os.path.sep.join([MODEL_PATH, \"coco.names\"])\n",
    "LABELS = open(labelsPath).read().strip().split(\"\\n\")\n",
    "\n",
    "# derive the paths to the YOLO weights and model configuration\n",
    "weightsPath = os.path.sep.join([MODEL_PATH, \"yolov3.weights\"])\n",
    "configPath = os.path.sep.join([MODEL_PATH, \"yolov3.cfg\"])\n",
    "\n",
    "# Load the serialized pretrained model\n",
    "net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)\n",
    "\n",
    "# check if we are going to use GPU\n",
    "if USE_GPU:\n",
    "    # set CUDA as the preferable backend and target\n",
    "    print(\"Settting backend and target to CUDA...\")\n",
    "    net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "    net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\n",
    "    \n",
    "# determine only the output layer names that we need from YOLO detector\n",
    "ln = net.getLayerNames()\n",
    "ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Streaming the input video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the ID for \"person\" class\n",
    "personIdx = LABELS.index(\"person\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a VideoCapture object\n",
    "cap = cv2.VideoCapture(input_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum distance (in pixels) for implementing social distancing\n",
    "num_iter = 20\n",
    "MIN_DISTANCE = get_min_dist(input_video, num_iter, net, ln, personIdx)\n",
    "print(\"Minimum Pixel to set threshold for Social distancing violation is {}\".format(MIN_DISTANCE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We convert the resolutions from float to integer.\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "# Define the codec and create VideoWriter object.The output is stored in outputfile file.\n",
    "out = cv2.VideoWriter(output_file, cv2.VideoWriter_fourcc('M','J','P','G'), 10, (frame_width,frame_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Social Distance Monitoring Tracker consist of number of people detected, number of people violated and social distancing Index\n",
    "tracker = []\n",
    "# loop over the frames from the video stream\n",
    "while True:\n",
    "    # read the next frame from the file\n",
    "    (grabbed, frame) = cap.read()\n",
    "\n",
    "    # if the frame was not grabbed, then we have reached the end of the stream\n",
    "    if not grabbed:\n",
    "        break\n",
    "\n",
    "    # resize the frame and then detect people (and only people) in it\n",
    "    frame = imutils.resize(frame, width=700)\n",
    "    results = detect_people(frame, net, ln, personIdx)\n",
    "\n",
    "    # initialize the set of indexes that violate the minimum social distance\n",
    "    violate = set()\n",
    "\n",
    "    # ensure there are *at least* two people detections (required in order to compute our pairwise distance maps)\n",
    "    if len(results) >= 2:\n",
    "        centroids = np.array([r[2] for r in results])\n",
    "        D = dist.cdist(centroids, centroids, metric=\"euclidean\")\n",
    "\n",
    "        # loop over the upper triangular of the distance matrix\n",
    "        for i in range(0, D.shape[0]):\n",
    "            for j in range(i + 1, D.shape[1]):\n",
    "                if D[i, j] < MIN_DISTANCE:\n",
    "                    violate.add(i)\n",
    "                    violate.add(j)\n",
    "\n",
    "    # loop over the results\n",
    "    for (i, (prob, bbox, centroid)) in enumerate(results):\n",
    "        # extract the bounding box and centroid coordinates, then initialize the color of the annotation\n",
    "        (startX, startY, endX, endY) = bbox\n",
    "        (cX, cY) = centroid\n",
    "        color = (255, 0, 0)\n",
    "\n",
    "        # if the index pair exists within the violation set, then update the color\n",
    "        if i in violate:\n",
    "            color = (0, 0, 255)\n",
    "\n",
    "        # draw (1) a bounding box around the person and (2) the centroid coordinates of the person\n",
    "        cv2.rectangle(frame, (startX, startY), (endX, endY), color, 1)\n",
    "        cv2.circle(frame, (cX, cY), 3, color, 1)\n",
    "\n",
    "    # draw the total number of social distancing violations on the output frame\n",
    "    font_scale = 1\n",
    "    font = cv2.FONT_HERSHEY_PLAIN\n",
    "    # set the rectangle background to white\n",
    "    rectangle_bgr = (255, 255, 255)\n",
    "    # Social Distancing Index scale 1 Good; 0 Poor\n",
    "    num_p = len(results)\n",
    "    num_v = len(violate)\n",
    "    SDidx = 1 - (num_v/num_p)\n",
    "    SDidx = np.round(SDidx, 2)\n",
    "    r = (num_p, num_v, SDidx)\n",
    "    tracker.append(r)\n",
    "    # get the width and height of the text box\n",
    "    (text_width, text_height) = cv2.getTextSize(text, font, fontScale=font_scale, thickness=1)[0]\n",
    "    # set the text start position\n",
    "    text_offset_x = 10\n",
    "    text_offset_y = frame.shape[0] - 25\n",
    "    # make the coords of the box with a small padding of two pixels\n",
    "    box_coords = ((text_offset_x, text_offset_y), (text_offset_x + text_width + 3, text_offset_y - text_height - 1))\n",
    "    cv2.rectangle(frame, box_coords[0], box_coords[1], rectangle_bgr, cv2.FILLED)\n",
    "    cv2.putText(frame, text, (text_offset_x, text_offset_y), font, fontScale=font_scale, color=(0, 0, 0), thickness=1)\n",
    "    # Write the frame into the output file\n",
    "    out.write(frame)\n",
    "    # check to see if the output frame should be displayed to our screen\n",
    "    if display > 0:\n",
    "        # show the output frame\n",
    "        cv2.imshow(\"Frame\", frame)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        # if the `q` key was pressed, break from the loop\n",
    "        if key == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "# When everything done, release the video capture and video write objects\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "# Closes all the frames\n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
